#!/usr/bin/env python3
"""
compare_benchmarks.py - æ€§èƒ½å›å½’æ£€æµ‹

æ¯”è¾ƒä¸¤æ¬¡åŸºå‡†æµ‹è¯•ç»“æœï¼Œæ£€æµ‹æ€§èƒ½å›å½’å¹¶ç”ŸæˆæŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•:
    python3 compare_benchmarks.py <baseline.json> <current.json> --threshold 10 --output report.md

è¾“å‡º:
    - Markdown æ ¼å¼çš„æ¯”è¾ƒæŠ¥å‘Š
    - è¶…è¿‡é˜ˆå€¼çš„å›å½’å°†è¢«æ ‡è®°
    - å¦‚æœæ£€æµ‹åˆ°å›å½’ï¼Œé€€å‡ºç ä¸º 1
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, List


class BenchmarkComparison:
    """åŸºå‡†æµ‹è¯•æ¯”è¾ƒå™¨"""

    def __init__(self, threshold_percent: float = 10.0):
        """
        Args:
            threshold_percent: å›å½’æ£€æµ‹é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
        """
        self.threshold_percent = threshold_percent
        self.improvements = []
        self.regressions = []
        self.unchanged = []
        self.new_tests = []
        self.removed_tests = []

    def compare(
        self,
        baseline: Dict[str, Any],
        current: Dict[str, Any]
    ) -> Tuple[bool, str]:
        """
        æ¯”è¾ƒåŸºå‡†æµ‹è¯•ç»“æœ

        Args:
            baseline: åŸºçº¿ç»“æœ JSON
            current: å½“å‰ç»“æœ JSON

        Returns:
            (has_regression, report_markdown)
        """
        baseline_benchmarks = baseline.get("benchmarks", {})
        current_benchmarks = current.get("benchmarks", {})

        # è·å–æ‰€æœ‰æµ‹è¯•åç§°
        all_tests = set()
        for bench_name, tests in baseline_benchmarks.items():
            for test_name in tests:
                all_tests.add((bench_name, test_name))

        for bench_name, tests in current_benchmarks.items():
            for test_name in tests:
                all_tests.add((bench_name, test_name))

        # é€ä¸ªæ¯”è¾ƒ
        for bench_name, test_name in sorted(all_tests):
            baseline_val = self._get_value(baseline_benchmarks, bench_name, test_name)
            current_val = self._get_value(current_benchmarks, bench_name, test_name)

            if baseline_val is None and current_val is not None:
                self.new_tests.append((bench_name, test_name, current_val))
            elif baseline_val is not None and current_val is None:
                self.removed_tests.append((bench_name, test_name, baseline_val))
            elif baseline_val is not None and current_val is not None:
                change_percent = self._calc_change(baseline_val["value"], current_val["value"])

                comparison = {
                    "bench": bench_name,
                    "test": test_name,
                    "baseline": baseline_val["value"],
                    "current": current_val["value"],
                    "unit": current_val.get("unit", "msecs"),
                    "change": change_percent
                }

                # æ³¨æ„ï¼šå¯¹äºæ—¶é—´æŒ‡æ ‡ï¼Œå¢åŠ æ˜¯å›å½’ï¼Œå‡å°‘æ˜¯æ”¹è¿›
                if change_percent > self.threshold_percent:
                    self.regressions.append(comparison)
                elif change_percent < -self.threshold_percent:
                    self.improvements.append(comparison)
                else:
                    self.unchanged.append(comparison)

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(baseline, current)
        has_regression = len(self.regressions) > 0

        return has_regression, report

    def _get_value(
        self,
        benchmarks: Dict,
        bench_name: str,
        test_name: str
    ) -> Dict[str, Any] | None:
        """è·å–æŒ‡å®šæµ‹è¯•çš„å€¼"""
        if bench_name not in benchmarks:
            return None
        if test_name not in benchmarks[bench_name]:
            return None
        return benchmarks[bench_name][test_name]

    def _calc_change(self, baseline: float, current: float) -> float:
        """è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”"""
        if baseline == 0:
            return 0.0
        return ((current - baseline) / baseline) * 100

    def _generate_report(
        self,
        baseline: Dict[str, Any],
        current: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        lines = []

        # å…ƒæ•°æ®
        baseline_meta = baseline.get("metadata", {})
        current_meta = current.get("metadata", {})

        lines.append("### ğŸ“ˆ æ€§èƒ½æ¯”è¾ƒæ¦‚è§ˆ\n")
        lines.append(f"| é¡¹ç›® | åŸºçº¿ | å½“å‰ |")
        lines.append(f"|------|------|------|")
        lines.append(f"| ç‰ˆæœ¬ | `{baseline_meta.get('version', 'N/A')}` | `{current_meta.get('version', 'N/A')}` |")
        lines.append(f"| æ—¶é—´ | {baseline_meta.get('timestamp', 'N/A')[:19]} | {current_meta.get('timestamp', 'N/A')[:19]} |")
        lines.append("")

        # æ‘˜è¦ç»Ÿè®¡
        total = len(self.improvements) + len(self.regressions) + len(self.unchanged)
        lines.append("### ğŸ“Š ç»Ÿè®¡æ‘˜è¦\n")
        lines.append(f"- âœ… æ”¹è¿›: **{len(self.improvements)}** ä¸ªæµ‹è¯•")
        lines.append(f"- âŒ å›å½’: **{len(self.regressions)}** ä¸ªæµ‹è¯•")
        lines.append(f"- â– æ— å˜åŒ–: **{len(self.unchanged)}** ä¸ªæµ‹è¯•")
        lines.append(f"- ğŸ†• æ–°å¢: **{len(self.new_tests)}** ä¸ªæµ‹è¯•")
        lines.append(f"- ğŸ—‘ï¸ ç§»é™¤: **{len(self.removed_tests)}** ä¸ªæµ‹è¯•")
        lines.append(f"- ğŸ“ é˜ˆå€¼: Â±{self.threshold_percent}%")
        lines.append("")

        # å›å½’è­¦å‘Š
        if self.regressions:
            lines.append("### âš ï¸ REGRESSION DETECTED - æ€§èƒ½å›å½’\n")
            lines.append("ä»¥ä¸‹æµ‹è¯•çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼š\n")
            lines.append("| åŸºå‡†æµ‹è¯• | æµ‹è¯•ç”¨ä¾‹ | åŸºçº¿ | å½“å‰ | å˜åŒ– |")
            lines.append("|---------|---------|------|------|------|")
            for r in sorted(self.regressions, key=lambda x: x["change"], reverse=True):
                change_str = f"+{r['change']:.1f}%" if r['change'] > 0 else f"{r['change']:.1f}%"
                lines.append(
                    f"| {r['bench']} | {r['test']} | "
                    f"{r['baseline']:.2f} {r['unit']} | "
                    f"{r['current']:.2f} {r['unit']} | "
                    f"ğŸ”´ {change_str} |"
                )
            lines.append("")

        # æ”¹è¿›
        if self.improvements:
            lines.append("### âœ… æ€§èƒ½æ”¹è¿›\n")
            lines.append("ä»¥ä¸‹æµ‹è¯•çš„æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼š\n")
            lines.append("| åŸºå‡†æµ‹è¯• | æµ‹è¯•ç”¨ä¾‹ | åŸºçº¿ | å½“å‰ | å˜åŒ– |")
            lines.append("|---------|---------|------|------|------|")
            for r in sorted(self.improvements, key=lambda x: x["change"]):
                change_str = f"{r['change']:.1f}%"
                lines.append(
                    f"| {r['bench']} | {r['test']} | "
                    f"{r['baseline']:.2f} {r['unit']} | "
                    f"{r['current']:.2f} {r['unit']} | "
                    f"ğŸŸ¢ {change_str} |"
                )
            lines.append("")

        # æ— å˜åŒ–ï¼ˆæŠ˜å ï¼‰
        if self.unchanged:
            lines.append("<details>")
            lines.append("<summary>â– æ— æ˜¾è‘—å˜åŒ–çš„æµ‹è¯•ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n")
            lines.append("| åŸºå‡†æµ‹è¯• | æµ‹è¯•ç”¨ä¾‹ | åŸºçº¿ | å½“å‰ | å˜åŒ– |")
            lines.append("|---------|---------|------|------|------|")
            for r in self.unchanged:
                change_str = f"+{r['change']:.1f}%" if r['change'] > 0 else f"{r['change']:.1f}%"
                lines.append(
                    f"| {r['bench']} | {r['test']} | "
                    f"{r['baseline']:.2f} {r['unit']} | "
                    f"{r['current']:.2f} {r['unit']} | "
                    f"{change_str} |"
                )
            lines.append("\n</details>\n")

        # æ–°å¢æµ‹è¯•
        if self.new_tests:
            lines.append("### ğŸ†• æ–°å¢æµ‹è¯•\n")
            lines.append("| åŸºå‡†æµ‹è¯• | æµ‹è¯•ç”¨ä¾‹ | å€¼ |")
            lines.append("|---------|---------|-----|")
            for bench, test, val in self.new_tests:
                lines.append(f"| {bench} | {test} | {val['value']:.2f} {val.get('unit', 'msecs')} |")
            lines.append("")

        # ç§»é™¤æµ‹è¯•
        if self.removed_tests:
            lines.append("### ğŸ—‘ï¸ ç§»é™¤æµ‹è¯•\n")
            lines.append("| åŸºå‡†æµ‹è¯• | æµ‹è¯•ç”¨ä¾‹ | ä¹‹å‰çš„å€¼ |")
            lines.append("|---------|---------|---------|")
            for bench, test, val in self.removed_tests:
                lines.append(f"| {bench} | {test} | {val['value']:.2f} {val.get('unit', 'msecs')} |")
            lines.append("")

        return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="æ¯”è¾ƒåŸºå‡†æµ‹è¯•ç»“æœï¼Œæ£€æµ‹æ€§èƒ½å›å½’"
    )
    parser.add_argument(
        "baseline",
        type=Path,
        help="åŸºçº¿ç»“æœ JSON æ–‡ä»¶"
    )
    parser.add_argument(
        "current",
        type=Path,
        help="å½“å‰ç»“æœ JSON æ–‡ä»¶"
    )
    parser.add_argument(
        "--threshold", "-t",
        type=float,
        default=10.0,
        help="å›å½’æ£€æµ‹é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼Œé»˜è®¤ 10ï¼‰"
    )
    parser.add_argument(
        "--output", "-o",
        type=Path,
        default=None,
        help="è¾“å‡º Markdown æŠ¥å‘Šè·¯å¾„"
    )
    parser.add_argument(
        "--fail-on-regression",
        action="store_true",
        default=True,
        help="æ£€æµ‹åˆ°å›å½’æ—¶è¿”å›éé›¶é€€å‡ºç "
    )

    args = parser.parse_args()

    # è¯»å–æ–‡ä»¶
    if not args.baseline.exists():
        print(f"é”™è¯¯: åŸºçº¿æ–‡ä»¶ä¸å­˜åœ¨: {args.baseline}")
        sys.exit(1)

    if not args.current.exists():
        print(f"é”™è¯¯: å½“å‰ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.current}")
        sys.exit(1)

    with open(args.baseline, 'r', encoding='utf-8') as f:
        baseline = json.load(f)

    with open(args.current, 'r', encoding='utf-8') as f:
        current = json.load(f)

    # æ¯”è¾ƒ
    comparator = BenchmarkComparison(threshold_percent=args.threshold)
    has_regression, report = comparator.compare(baseline, current)

    # è¾“å‡ºæŠ¥å‘Š
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(report)

    # é€€å‡ºç 
    if has_regression and args.fail_on_regression:
        print(f"\nâŒ æ£€æµ‹åˆ° {len(comparator.regressions)} ä¸ªæ€§èƒ½å›å½’ï¼")
        sys.exit(1)
    else:
        print(f"\nâœ“ æœªæ£€æµ‹åˆ°æ˜¾è‘—æ€§èƒ½å›å½’")
        sys.exit(0)


if __name__ == "__main__":
    main()
